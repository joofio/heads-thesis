@article{adadiPeekingBlackBoxSurvey2018,
  title = {Peeking {{Inside}} the {{Black-Box}}: {{A Survey}} on {{Explainable Artificial Intelligence}} ({{XAI}})},
  shorttitle = {Peeking {{Inside}} the {{Black-Box}}},
  author = {Adadi, Amina and Berrada, Mohammed},
  year  = {2018},
  journal = {IEEE Access},
  volume = {6},
  pages = {52138--52160},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2018.2870052},
  abstract = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
  eventtitle = {{{IEEE Access}}},
  keywords = {Biological system modeling,black-box models,Conferences,Explainable artificial intelligence,interpretable machine learning,Machine learning,Machine learning algorithms,Market research,Prediction algorithms}
}
@article{linardatosExplainableAIReview2020,
  title = {Explainable {{AI}}: {{A Review}} of {{Machine Learning Interpretability Methods}}},
  shorttitle = {Explainable {{AI}}},
  author = {Linardatos, Pantelis and Papastefanopoulos, Vasilis and Kotsiantis, Sotiris},
  year  = {2020-12-25},
  journal = {Entropy},
  shortjournal = {Entropy},
  volume = {23},
  number = {1},
  pages = {18},
  issn = {1099-4300},
  doi = {10.3390/e23010018},
  url = {https://www.mdpi.com/1099-4300/23/1/18},
  urlyear  = {2023-07-18},
  abstract = {Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into “black box” approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.},
  langid = {english}
}


@article{barredoarrietaExplainableArtificialIntelligence2020,
  title = {Explainable {{Artificial Intelligence}} ({{XAI}}): {{Concepts}}, Taxonomies, Opportunities and Challenges toward Responsible {{AI}}},
  shorttitle = {Explainable {{Artificial Intelligence}} ({{XAI}})},
  author = {Barredo Arrieta, Alejandro and Díaz-Rodríguez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garcia, Salvador and Gil-Lopez, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
  year  = {2020-06},
  journal = {Information Fusion},
  shortjournal = {Information Fusion},
  volume = {58},
  pages = {82--115},
  issn = {15662535},
  doi = {10.1016/j.inffus.2019.12.012},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1566253519308103},
  urlyear  = {2023-07-18},
  abstract = {In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.},
  langid = {english}
}
@book{kamath2021explainable,
  title = {Explainable Artificial Intelligence: {{An}} Introduction to Interpretable Machine Learning},
  author = {Kamath, U. and Liu, J.},
  year  = {2021},
  publisher = {{Springer International Publishing}},
  url = {https://books.google.pt/books?id=_O2LzgEACAAJ},
  isbn = {978-3-030-83355-8}
}
@article{rudinStopExplainingBlack2019,
  title = {Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead},
  author = {Rudin, Cynthia},
  year  = {2019-05},
  journal = {Nature Machine Intelligence},
  shortjournal = {Nat Mach Intell},
  volume = {1},
  number = {5},
  pages = {206--215},
  publisher = {{Nature Publishing Group}},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0048-x},
  url = {https://www.nature.com/articles/s42256-019-0048-x},
  urlyear  = {2023-07-18},
  abstract = {Black box machine learning models are currently being used for high-stakes decision making throughout society, causing problems in healthcare, criminal justice and other domains. Some people hope that creating methods for explaining these black box models will alleviate some of the problems, but trying to explain black box models, rather than creating models that are interpretable in the first place, is likely to perpetuate bad practice and can potentially cause great harm to society. The way forward is to design models that are inherently interpretable. This Perspective clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare and computer vision.},
  issue = {5},
  langid = {english},
  keywords = {Computer science,Criminology,Science,Statistics,technology and society}
}
@article{panchInconvenientTruthAI2019,
  title = {The ``Inconvenient Truth'' about {{AI}} in Healthcare},
  author = {Panch, Trishan and Mattie, Heather and Celi, Leo Anthony},
  year = {2019},
  month = aug,
  journal = {npj Digital Medicine},
  volume = {2},
  number = {1},
  pages = {77},
  issn = {2398-6352},
  doi = {10.1038/s41746-019-0155-4},
  urldate = {2023-07-20},
  langid = {english},
  note = {\url{https://www.nature.com/articles/s41746-019-0155-4}}
}
@article{peekThreeControversiesHealth2018,
  title = {Three Controversies in Health Data Science},
  author = {Peek, Niels and Rodrigues, Pedro Pereira},
  year = {2018},
  month = nov,
  journal = {International Journal of Data Science and Analytics},
  volume = {6},
  number = {3},
  pages = {261--269},
  issn = {2364-4168},
  doi = {10.1007/s41060-018-0109-y},
  urldate = {2021-03-28},
  langid = {english},
  note = {\url{https://doi.org/10.1007/s41060-018-0109-y}}
}
@article{abul-husnPersonalizedMedicinePower2019,
  title = {Personalized Medicine and the Power of Electronic Health Records},
  author = {{Abul-Husn}, Noura S. and Kenny, Eimear E.},
  year = {2019},
  month = mar,
  journal = {Cell},
  volume = {177},
  number = {1},
  pages = {58--69},
  issn = {0092-8674},
  doi = {10.1016/j.cell.2019.02.039},
  urldate = {2023-07-20},
  pmcid = {PMC6921466},
  pmid = {30901549},
  note = {\url{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6921466/}}
}
@article{adler-milsteinHITECHActDrove2017,
  title = {{{HITECH Act Drove Large Gains In Hospital Electronic Health Record Adoption}}},
  author = {{Adler-Milstein}, Julia and Jha, Ashish K.},
  year = {2017},
  month = aug,
  journal = {Health Affairs},
  volume = {36},
  number = {8},
  pages = {1416--1422},
  issn = {0278-2715, 1544-5208},
  doi = {10.1377/hlthaff.2016.1651},
  urldate = {2023-07-20},
  langid = {english},
  note = {\url{http://www.healthaffairs.org/doi/10.1377/hlthaff.2016.1651}}
}
@article{kruseUseElectronicHealth2018,
  title = {The Use of {{Electronic Health Records}} to {{Support Population Health}}: {{A Systematic Review}} of the {{Literature}}},
  shorttitle = {The Use of {{Electronic Health Records}} to {{Support Population Health}}},
  author = {Kruse, Clemens Scott and Stein, Anna and Thomas, Heather and Kaur, Harmander},
  year = {2018},
  month = nov,
  journal = {Journal of Medical Systems},
  volume = {42},
  number = {11},
  pages = {214},
  issn = {0148-5598, 1573-689X},
  doi = {10.1007/s10916-018-1075-6},
  urldate = {2023-07-20},
  langid = {english},
  note = {\url{http://link.springer.com/10.1007/s10916-018-1075-6}}
}
@article{palabindalaAdoptionElectronicHealth2016,
  title = {Adoption of Electronic Health Records and Barriers},
  author = {Palabindala, Venkataraman and Pamarthy, Amaleswari and Jonnalagadda, Nageshwar Reddy},
  year = {2016},
  month = jan,
  journal = {Journal of Community Hospital Internal Medicine Perspectives},
  volume = {6},
  number = {5},
  pages = {32643},
  issn = {2000-9666},
  doi = {10.3402/jchimp.v6.32643},
  urldate = {2023-07-20},
  langid = {english},
  note = {\url{https://www.tandfonline.com/doi/full/10.3402/jchimp.v6.32643}}
}
@techreport{sertkayaaylinEXAMINATIONCLINICALTRIAL2014,
  title = {{{Examination of Clinical Trial Costs and Barriers for Drug Development}}},
  author = {{Sertkaya,Aylin} and {Birkenbach, Anna} and {Berlind, Ayesha} and {Eyraud, John}},
  year = {2014},
  month = jul,
  institution = {{Eastern Research Group, Inc.}},
  note = {\url{https://aspe.hhs.gov/sites/default/files/private/pdf/77166/rpt_erg.pdf}}
}
@article{michelsonSignificantCostSystematic2019,
  title = {The Significant Cost of Systematic Reviews and Meta-Analyses: {{A}} Call for Greater Involvement of Machine Learning to Assess the Promise of Clinical Trials},
  shorttitle = {The Significant Cost of Systematic Reviews and Meta-Analyses},
  author = {Michelson, Matthew and Reuter, Katja},
  year = {2019},
  month = aug,
  journal = {Contemporary Clinical Trials Communications},
  volume = {16},
  pages = {100443},
  issn = {2451-8654},
  doi = {10.1016/j.conctc.2019.100443},
  urldate = {2023-07-20},
  pmcid = {PMC6722281},
  pmid = {31497675},
  note = {\url{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6722281/}}
}
