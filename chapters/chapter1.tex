\begin{savequote}[75mm]
If we knew what it was we were doing, it would not be called research, would it?
\qauthor{Albert Einstein}
\end{savequote}
\chapter{Introduction} \label{chap:intro}

%reset de nomeclaturas
\acresetall

The practice of healthcare is deeply intertwined with technological advancements. Technology, in its broadest definition, encompasses \textit{"methods, systems, and devices which are the result of scientific knowledge being used for practical purposes"}. In essence, healthcare and medicine represent applied sciences, utilizing principles from biology, physics, chemistry, and mathematics to develop treatments, diagnostic methods, and medical procedures. Over the past two to three decades, the fields of computer science and informatics have increasingly integrated into the healthcare domain, significantly influencing its evolution and methodologies. \cite{adler-milsteinHITECHActDrove2017}. A paper-based industry is now being digitalized and computerized. This has been leading to an increase in the amount of data generated by healthcare systems \cite{kruseUseElectronicHealth2018,palabindalaAdoptionElectronicHealth2016}. 
This data has the potential to greatly improve the current methods and practices in healthcare. However, it is still not being used to its full potential \cite{kruseUseElectronicHealth2018,dicamilloGuestEditorialData2020}. This is especially important when we note that the gold standard of evidence creation is \acp{rct} which can vary on quality, time, and resources. A \ac{rct} may cost no less than 20 million euros to run, and according to a report submitted to the \ac{us} Department of Health and Human Services \cite{sertkayaaylinEXAMINATIONCLINICALTRIAL2014} can cost as much as 100 million \ac{us} dollars. This is indeed a very steep price to get the information we need to innovate. Parallel to this, usually supported by these \acp{rct} are systematic reviews and meta-analyses, highly supported and promoted by \ac{ebm} which are estimated to cost around 140 thousand dollars each \cite{michelsonSignificantCostSystematic2019}. Additionally, we must take into account the time that it takes to create and publish a good paper on evidence synthesis, often making it hard to keep up with the pace of innovation.

So, we are now being faced with huge amounts of clinical data generated by \acp{ehr} and \acp{his}. But which tools are the most suited for harvesting the potential of this data? 
The capabilities and assumptions behind modern \ac{kdd}, \ac{ml}, and \ac{ai} seem like a good approach for harvesting this potential. However, they are very different from the traditional statistical methods that are usually used in healthcare.
So, in order to properly use these methods in healthcare and actually provide value to the patients, we need to understand the differences between these methods and how they can be used to complement each other.

Currently, we already have an idea of what are the major key areas that hinder the adoption of \ac{ai} in healthcare like problems related to data privacy and security, data quality and integrity, interoperability, ethical considerations, and the fact that the hype of \ac{ai} is far greater than the \ac{ai} science, the acceptance, and trust of healthcare practitioners of \ac{ai} based systems \cite{muhiyaddinImpactClinicalDecision2020,kilsdonkFactorsInfluencingImplementation2017}, and how to proper evaluate the potential risks of \ac{ai} in healthcare, just to mention a few \cite{topolHighperformanceMedicineConvergence2019a}.
This is a very complex problem that requires a lot of different approaches and solutions. It is a popular assumption that 87\% of data science projects never get into production \cite{Why87Data2019}. Even if numbers for the healthcare domain are not available at this time, it is safe to assume that the number is not much different, if not higher. And those that actually do, may never actually create any impact due to the lack of adoption by the healthcare practitioners or the lack of trust in the system \cite{walkerModelGuidedDecisionMakingThromboprophylaxis2023}.

So, with this introduction, we have there is still a long way to go to harvest all the potential healthcare data has to offer. And so, our research objectives are focused on powering up this adoption. What can be done to improve these chances? What can we bring to the table to enhance the rate of success?

%cost of systematics review
%https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6722281/

%cost of trials
%https://www.sofpromed.com/how-much-does-a-clinical-trial-cost


%%% falar de perfil hibrido e saber de saude e de machine leraning e estatistica sera complicado


%%falar da pouca adoção de ML e CDSS
%%%TODO finish

\section{Research Objectives}
%%rework
This thesis has three main goals:


\begin{itemize}
    \item Goal 1: Research methods to improve data quality, whether using synthetic data generation to enlarge data volume and protect privacy (sections \ref{subsec:gans}, \ref{subsec:tabular} and \ref{subsec:similarity}) or by creating automatic data quality assessment methods (section \ref{subsec:dq})

    \item Goal 2: Assess health data science  methods with limited data access (sections \ref{subsec:distributed} and \ref{subsec:benchmark}).

    \item Goal 3: Explore strategies to transform health data into actionable decisions and policies (sections  \ref{subsec:ipop} and \ref{subsec:obs}).
\end{itemize}


\section{Research Questions}

\begin{itemize}
    \item Goal 1: How can we improve the quality of data used in health data science?
    \begin{itemize}
        \item RQ1.1: Can GANs Help Create Realistic Datasets?
        \item RQ1.2: How Can We Compare Two Tabular Datasets?
        \item RQ1.3: Can We Use Machine Learning Feature to Compare Datasets?
        \item RQ1.4: Can We Use Machine Learning to Create Automatic Data Quality Assessments?
    \end{itemize}
\end{itemize}

For this goal, our aim is to improve the quality of data used in health data science. We will start by exploring the use of \ac{gan} to create realistic tabular datasets (section \ref{subsec:gans}). We have seen the advent and deep learning in creating data, namely for image, video and sound. From these examples, \acp{gan} have been providing excellent results, but do we know if this performance is matched in tabular datasets? Then we will explore the methods for comparing datasets (section \ref{subsec:tabular}), which relates to the first point since we cannot create good synthetic data if we do not have good metrics to assess how equal they are. So we tried to compile the current state-of-the-art metrics for such. And since we did not find, in our opinion, a good set of metrics in the 2nd point, we tried to apply \ac{ml} to create a good evaluation metrics for comparing two tabular datasets (section \ref{subsec:similarity}). Finally,  we will then explore the use of \ac{ml} to create automatic data quality assessments. This is a very important point since we need to know if the data we are using is good enough for the task at hand. This is especially important when we are using \ac{ml} methods since they are very sensitive to data quality (section \ref{subsec:dq}).

\begin{itemize}

    \item Goal 2: How can we assess health data science methods with limited data access?
    \begin{itemize}
        \item RQ2.1: Leveraging Distributed Systems in Healthcare: is it Advisable?
        \item RQ2.2: Can Institutions Share Their Performance Metrics Without Hesitation of Retaliation?
    \end{itemize}
\end{itemize}

For this goal, we tried to overcome the data access limitations. If we do not have access to data, how can we explore its potential? There are several issues that are set in place to guarantee the good, safe and ethical usage of data, but sometimes it creates hurdles to a timely usage of the data. So we tried to evaluate if distributed learning was a fitting option to overcome this limitation (section \ref{subsec:distributed}). Then we tried to come up with an alternative to compare health institutions without actually sharing their true metrics, but still be able to position themselves in a scale (section \ref{subsec:benchmark}). 

\begin{itemize}
    \item Goal 3: How can we convert health data into decisions and policies?
    \begin{itemize}
        \item RQ3.1: How Can We Leverage Data to Create Clinical Decision Support Systems?
        \item RQ3.2: How Can We Leverage Data to Assess Treatment Efficacy?
    \end{itemize}    
\end{itemize}

For the last goal, we tried to actually go from end-to-end. This means that we tried to leverage real world data in its raw form and transform it into actionable insights. Our major objective was to check the challenges that block the development of such tools. We tried to assess the real-world effect of two drugs for breast cancer and compare them among themselves and with the previous gold standard: endocrine therapy (section \ref{subsec:ipop}). Then we tried to create a \ac{cdss} that could be used in real-time clinical environments and be able to provide support for the need of C-sections (section \ref{subsec:obs}). 