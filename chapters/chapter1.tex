% !TeX root = ../thesis.tex


\begin{savequote}[85mm]
    Nothing great in the world was accomplished 
    without passion.
    \qauthor{Friedrich Hegel}
    \end{savequote}


\chapter{Introduction} \label{chap:intro}

%reset de nomeclaturas
\acresetall

%\doublespacing
\initial{I} first started thinking about the topic of this thesis during a class of the PhD program called precisely HEADS - Health Data Science. The class concerned the gap between the availability and usage of health data for secondary use and how this was a complex issue. At the time, Professor Pedro Rodrigues raised three big questions that were (are) still open in the field. The first was the reuse of data, and if it was possible, ethical, and feasible to use data that was created for another purpose for secondary use - research. The second was about the extent to which routine data sources and innovations in analytical methods alleviate the need to conduct randomized clinical trials. The last was a question related to matters of governance, privacy, and trust when routine health data is made available for research.

I spent some time on this subject and decided to tackle it. This decision was motivated by the fact that these subjects relate to core challenges for research with health data and define an essential research agenda for the health data science community. Furthermore, I also believe that these subjects are related to the slow adoption of innovative digital and informatics solutions in clinical settings, which often results in significant losses in the potential benefits of these technologies. Despite groundbreaking discoveries in the industry and academic settings, there seems to be a substantial gap in translating these advancements into practical industry applications. This realization sparked a question: how could we effectively bridge this gap and fully leverage the potential of healthcare data?\\

%This outline will follow these three questions.\\
%The genesis of this thesis was rooted in a profound curiosity about the potential for enhancing clinical knowledge through rapid, high-quality technological advancements. As I delved into the interplay between healthcare and technology, I noticed a prevalent issue: the data that 



%the slow adoption of innovative digital and informatics solutions in clinical settings, which often resulted in significant losses in the potential benefits of these technologies. Despite groundbreaking discoveries in academia, there seemed to be a substantial gap in translating these advancements into practical industry applications. This realization sparked a question: how could we effectively bridge this gap and fully leverage these technologies in healthcare?
%To address this, the thesis is structured as follows:\\

We do know that healthcare practices are deeply intertwined with technological advancements. Technology, in its broadest definition, encompasses \textit{"methods, systems, and devices which are the result of scientific knowledge being used for practical purposes"}. In essence, healthcare and medicine represent applied sciences, utilizing principles from biology, physics, chemistry, and mathematics to develop treatments, diagnostic methods, and medical procedures. Over the past two to three decades, the fields of computer science and informatics have increasingly integrated into the healthcare domain, significantly influencing its evolution and methodologies \cite{adler-milsteinHITECHActDrove2017}. A paper-based industry is now being digitalized and computerized, harnessing its potential. This has been leading to an increase in the amount of data generated by healthcare systems \cite{kruseUseElectronicHealth2018,palabindalaAdoptionElectronicHealth2016}.

These data have the potential to greatly improve the current methods and practices in healthcare. However, they are still not being used to their full potential \cite{kruseUseElectronicHealth2018,dicamilloGuestEditorialData2020}. But why is that?

The questions raised in that class of 2019, published in \cite{peekThreeControversiesHealth2018}, seem to sum it up quite well.

\begin{itemize}
    \item Van der Lei’s First Law of Medical Informatics \cite{vanderleiUseAbuseComputerstored1991}: Data shall be used only for the purpose for which they were collected. Is it advisable to reuse data from \acp{ehr} for research?
    \item Effectiveness of Routine Data Sources and Analytical Innovations: Examining the extent to which routine data sources and innovations in analytical methods alleviate the need for randomized clinical trials. Can statistics and big data on observational data replace \acp{rct} and provide insight into causality?
    \item Governance, Privacy, and Trust Issues: Addressing governance, privacy, and trust questions when routine health data are made available for research. Are patient's comfortable with the reuse of their data?

\end{itemize}



%Taking a deeper look to them, we can see that there is more than meets the eye. 


This is especially important when we note that the gold standard for evidence creation is \acp{rct}, which can vary in quality, time, and resources. A \ac{rct} may cost no less than 20 million euros to run, and according to a report submitted to the \ac{us} Department of Health and Human Services \cite{sertkayaaylinEXAMINATIONCLINICALTRIAL2014} can cost as much as 100 million \ac{us} dollars. This is indeed a very steep price for obtaining the information we need to innovate. Parallel to this, usually supported by these \acp{rct} are systematic reviews and meta-analyses, highly supported and promoted by \ac{ebm} which are estimated to cost approximately 140 thousand dollars each \cite{michelsonSignificantCostSystematic2019}. Additionally, we must take into account the time it takes to create and publish a good paper on evidence synthesis, often making it hard to keep up with the pace of innovation.

So, we are now being faced with huge amounts of clinical data generated by \acp{ehr} and \acp{his}. But which tools are the most suited for harvesting the potential of this data? 
The capabilities and assumptions behind modern \ac{kdd}, \ac{ml}, and \ac{ai} seem to be good approaches for harvesting this potential. However, they are very different from the traditional statistical methods that are typically used in healthcare.
Therefore, to properly use these methods in healthcare and actually provide value to patients, we need to understand the differences between these methods and how they can be used to complement each other.
%expandir com controversiasas e depois dizer que mesmo que isso se resolva,temos ainda o x e y ...
Currently, we already have an idea of what are the major key areas that hinder the adoption of \ac{ai} in healthcare like problems related to data privacy and security, data quality and integrity, interoperability, ethical considerations, and the fact that the hype of \ac{ai} is far greater than the \ac{ai} science, the acceptance, and trust of healthcare practitioners of \ac{ai} based systems \cite{muhiyaddinImpactClinicalDecision2020,kilsdonkFactorsInfluencingImplementation2017}, and how to proper evaluate the potential risks of \ac{ai} in healthcare, just to mention a few \cite{topolHighperformanceMedicineConvergence2019a}.
This is a very complex problem that requires many approaches and solutions. It is a popular assumption that 87\% of data science projects never reach production \cite{Why87Data2019}. Even if numbers for the healthcare domain are not available at this time, it is safe to assume that the number is not much different, if not higher. Those that actually do may never actually create any impact owing to the lack of adoption by healthcare practitioners or the lack of trust in the system \cite{walkerModelGuidedDecisionMakingThromboprophylaxis2023}.

There is still a long way to go to harvest all the potential healthcare data has to offer, and our research objectives are focused on powering up this adoption. What can be done to improve these chances? What can be brought to the table to enhance the success rate?

%cost of systematics review
%https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6722281/

%cost of trials
%https://www.sofpromed.com/how-much-does-a-clinical-trial-cost


%%% falar de perfil hibrido e saber de saude e de machine leraning e estatistica sera complicado


%%falar da pouca adoção de ML e CDSS
%%%TODO finish

\section{Research Objectives}
%%rework
This thesis has three main goals:


\begin{itemize}
    \item Goal 1: Research methods to improve data quality, whether using synthetic data generation to enlarge data volume and protect privacy (sections \ref{subsec:gans}, \ref{subsec:tabular} and \ref{subsec:similarity}) or by creating automatic data quality assessment methods (section \ref{subsec:dq})

    \item Goal 2: Assess health data science  methods with limited data access (sections \ref{subsec:distributed} and \ref{subsec:benchmark}).

    \item Goal 3: Explore strategies to transform health data into actionable decisions and policies (sections  \ref{subsec:ipop} and \ref{subsec:obs}).
\end{itemize}


\section{Research Questions}

\begin{itemize}
    \item Goal 1: How can we improve the quality of data used in health data science?
    \begin{itemize}
        \item RQ1.1: Can GANs help create realistic datasets?
        \item RQ1.2: How Can we compare two tabular datasets?
        \item RQ1.3: Can we use machine learning feature's importance to compare datasets?
        \item RQ1.4: Can we use machine learning to create automatic data quality assessments?
    \end{itemize}
\end{itemize}

For this goal, we aim to improve the quality of data used in health data science. We will start by exploring the use of \ac{gan} to create realistic tabular datasets (Section \ref{subsec:gans}). The advent of deep learning has led to remarkable progress in generating data, particularly for images, videos, and sounds. From these examples, \acp{gan} have produced excellent results. However, can this performance be matched in tabular datasets?

Next, we will explore methods for comparing datasets (Section \ref{subsec:tabular}), which relates to the first point since we cannot create good synthetic data without having robust metrics to assess their similarity. Therefore, we have attempted to compile the current state-of-the-art metrics for this purpose.

Since we did not find, in our opinion, a comprehensive set of metrics in the second point, we sought to apply \ac{ml} to create effective evaluation metrics for comparing two tabular datasets (Section \ref{subsec:similarity}). Finally, we will explore the use of \ac{ml} to create automatic data quality assessments (Section \ref{subsec:dq}). This is crucial because we need to know if the data we are using is of sufficient quality for the task at hand. This is especially important when using \ac{ml} methods, as they are highly sensitive to data quality.

%our aim is to improve the quality of data used in health data science. We will start by exploring the use of \ac{gan} to create realistic tabular datasets (section \ref{subsec:gans}). We have seen the advent and deep learning in creating data, namely for image, video and sound. From these examples, \acp{gan} have been providing excellent results. However, can this performance be matched in tabular datasets? Then we will explore the methods for comparing datasets (section \ref{subsec:tabular}), which relates to the first point since we cannot create good synthetic data if we do not have good metrics to assess how equal they are. So we tried to compile the current state-of-the-art metrics for such. And since we did not find, in our opinion, a good set of metrics in the 2nd point, we tried to apply \ac{ml} to create a good evaluation metrics for comparing two tabular datasets (section \ref{subsec:similarity}). Finally,  we will then explore the use of \ac{ml} to create automatic data quality assessments. This is a very important point since we need to know if the data we are using is good enough for the task at hand. This is especially important when we are using \ac{ml} methods since they are very sensitive to data quality (section \ref{subsec:dq}).


\begin{itemize}

    \item Goal 2: How can we assess health data science methods with limited data access?
    \begin{itemize}
        \item RQ2.1: Leveraging distributed systems in healthcare: is it advisable?
        \item RQ2.2: Can institutions share their performance metrics without hesitation of retaliation?
    \end{itemize}
\end{itemize}

To achieve this goal, we sought to overcome data access limitations. If we do not have access to data, how can we explore its potential? Several limitations are in place to ensure the ethical, safe, and appropriate use of patient data, but sometimes these limitations create obstacles to timely data usage or even prevent access altogether. Therefore, we aimed to evaluate whether distributed learning is a viable option to overcome this limitation (Section \ref{subsec:distributed}).

Subsequently, we attempted to develop an alternative method for comparing health institutions performance metrics without knowing the true values of each metric, but still enabling them to position themselves on a scale (Section \ref{subsec:benchmark}).

\begin{itemize}
    \item Goal 3: How can we convert health data into decisions and policies?
    \begin{itemize}
        \item RQ3.1: How can we leverage data to create clinical decision support systems?
        \item RQ3.2: How can we leverage data to assess treatment efficacy?
    \end{itemize}    
\end{itemize}

For the last goal, we tried to actually go from end-to-end. This means that we tried to leverage real world data in its raw form and transform it into actionable insights. Our major objective was to check the challenges that block the development of such tools. We tried to assess the real-world effect of two drugs for breast cancer and compare them among themselves and with the previous gold standard: endocrine therapy (section \ref{subsec:ipop}). Then we tried to create a \ac{cdss} that could be used in real-time clinical environments and be able to provide support for the need of C-sections (section \ref{subsec:obs}). 

\section{Thesis Contributions}

\begin{itemize}
    \item \textbf{Data Quality Improvement:} The thesis investigates methods to enhance data quality, either through the generation of synthetic data to increase data volume and protect privacy or by creating automatic data quality assessment methods. \acl{gan} are analysed for the creation of realistic and non-sensitive datasets. The thesis provides a list of application \acl{gan} in healthcare and the metrics used. It also provides access to a method for creating a report that compiles all available metrics for easy evaluation of two datasets. Following this, the thesis adds a new alternative for comparing tabular datasets around the usage of ranking metrics, especially \acl{rbo}. Finally, adds a method that tries to compile several methods of assessing data quality in obstetrics, turning it into a single metric interoperable model.  
    \item \textbf{Distributed Data Analysis:} The thesis evaluates the application of distributed data analysis to enable secure, localized analyses, improving the speed and security of the process. The thesis compares distributed, centralised, and local learning models, finding that distributed analysis is a potential alternative to standard data analysis, having distributed methods having similar or better performance than local and centralised models. It also adds a potential alternative for institutions to share performance metrics with privacy by using clustering.
    \item \textbf{Clinical Decision Support:} The thesis investigates how the integration of real-world data into clinical practice can drive innovation and improve clinical outcomes. Examples are presented of the application of causality principles and Machine Learning models to assess the efficacy of Palbociclib and Ribocilib for breast cancer treatments and the development of a  \acl{cdss} for obstetrics using \acl{ml}.
    \item \textbf{Interdisciplinary Collaboration:} The thesis highlights the need for a collaborative approach with clinicians, who are the end users of the developed tools. Understanding their needs and workflows is essential to develop user-friendly tools that clinicians can integrate into their practice. It also highlights the clear important of explainable \ac{ai} and diving deeper into causality models in order to fast-track the adoption of these tools by clinicians and health institutions.
\end{itemize}

The thesis advocates for the creation of robust data infrastructures, interdisciplinary collaboration, and a balanced legal and technical framework to drive digital health innovation.
