


The imputation process was done using the mean value (for continuous variables) or a new category (NULLIMP) for categorical variables. All categories were encoded as numbers using a previous mapping created based on all possible categories in all silos. Even though an ordinal relationship is created among features, we believe that since we are applying this methodology to all datasets, which will be the source for all tests (local, distributed and centralised), that fact may be ignored.
When training classification models, all of the target variable classes must be known at that moment and should be present in each split of the cross-validation. So, when assessing the training dataset, low-frequency target classes (n $<$ 25) were up-sampled with \ac{smote} \cite{smote} and missing target classes were addressed with dummy rows creation by the imputation of the mean for continuous variables and mode for categorical variables (per silo). These preprocessing mechanisms were applied in each run and for each target.
The distributed model was an ensemble of models from each silo on a weighted soft-voting basis, defining weights and thresholds based on the training set scores. 
The first thing that is noticeable is the high scores achieved in our analysis which show that all algorithms in all forms (local, distributed and centralised) have a good grasp on ranking data (negative on the bottom and positives on the top of a scale) for classification or predicting the value for regression. We notice that distributed models have performance similar to their centralised counterparts. $\sim$59\% of all of the distributed models had similar or better performance than the centralised models. This suggests that a distributed model can be used to reliably infer information and does not compromise prediction performance when compared with the gold standard (centralised) while increasing privacy for the data owners.\\
Overall, our results suggest that it is possible to implement a distributed model without significantly losing information. However, there are still issues to be addressed. This methodology presents hurdles regarding categorical class handling. Firstly, all classes should be known first-hand and should be given to each model even if that silo in particular has no cases of that class. Secondly, low-frequency classes are also an issue to be addressed, since training the model with cross-validation will raise problems because each split should have all classes present. Our approach relied on sample creation for low and non-existent target classes. However, this approach is adding information to the model that is not originally there. The way we chose for minimising this issue was by creating dummy variables with median and mode imputations based only on the information in the dataset. Nevertheless, non-existent classes are impossible to address without prior information. These class problems could be partially tackled in production by implementing data management and governance procedures, namely data dictionaries. Still on data preprocessing, we applied ordinal encoding to the variables which will create a natural hierarchy between variables. One solution for this is to create binary columns for each class in each column. This will remove the hierarchy between classes but increase variable numbers and training time considerably.\\
Moreover, like in most secondary usage of data, other issues are important to keep in mind, even in such a controlled environment as this one. Even though the software is the same in every hospital, the clinical service is the same and the underlying data models are the same, the version of the software is not the same across all hospitals. This difference alone can alter the way each column is populated, mainly through front-end changes or label modification, among other aspects. Additionally, each hospital has its own workflows in practice that can also alter the way data is collected; changing timings or steps in a certain workflow can dramatically change the data acquisition and the reality it represents. \\
Another issue to consider is the path adopted to build the distributed model. In this case, it was decided to develop an ensemble of models with voting. However, other methods could have been employed, like parameter averaging, that should be tested as well. In particular, the usage of more robust neural networks could be assessed as well. We chose not to test state-of-the-art neural networks since the data volume was low for that use case and several papers have already demonstrated that neural networks are not the most suitable tool for tabular data \cite{grinsztajnWhyTreebasedModels2022,borisovDeepNeuralNetworks2022}. We chose to add MLPerceptron as a baseline for comparison with the remaining algorithms. The results show us that the performance was below the other algorithms, but in this concrete case, the problem may reside in the architecture chosen and hyperparameters used in the Cross-validation. Despite this, a precise and thorough demonstration of this use case would be important to consider such scenarios. \\
Furthermore, the algorithm underlying the distributed model is of importance as well for its performance versus the centralised model. Figures \ref{fig:heatmap-cat} and \ref{fig:heatmpa-int} and table \ref{tab:hyp} show us that Decision trees and K-nearest neighbours implemented in a centralised manner are consistently better than the distributed counterpart. Even though this improvement may have a relationship to the target variable (i.e figure \ref{fig:heatmpa-int} for IA and IGA variables), it is still an important fact to take into account when implementing such architectures. The performance of the models is also interesting to catch differences in silos. See silo 6 for TPNP (figure \ref{fig:heatmap-cat}) where silo 6 consistently behaves differently than the rest.
As for implementation, such a mechanism could be implemented in at least two manners; with a central orchestrator or without. The first one would assume a central point that would make a request to each silo for a prediction and then create the final prediction with the weighted averaging of each one. The second one would not require any additional platform and each silo would communicate with each of the others and receive the prediction and would create the final with their own. This implementation step would of course take into account variables that we were out of scope such as the communication between silos. 
Regarding the prediction capability as a whole, we found that this data is suitable to apply machine-learning models in order to predict several clinical outcomes, with very good results for several target variables. 

%Another topic that can be explored is the fact that a lower performance of the ensemble in a certain silo and/or target could be actually useful to characterise the population of that silo.
