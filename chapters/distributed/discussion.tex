% !TeX root = ../../thesis.tex



%Our analysis reveals notably high performance across all tested algorithms, regardless of their deployment in local, distributed, or centralised settings. This is particularly evident in the realm of classification algorithms, where models consistently achieve an average of approximately 70\% for both AUROC and AUPRC. In the case of regression models, we frequently observed a Mean Absolute Error (MAE) lower than 1 for targets with an average value in the range of 20-30. 
A significant finding is that nearly 59\% of distributed models demonstrated comparable, if not superior, performance relative to their centralized counterparts (table \ref{tab:hyp} last column, first two values for each algorithm). From these, 41.9\% were also better or equal to the local model. If we take the best performing algorithm (SGD), we have 77.2\% for distributed better than centralised and 66\% better than centralized and local. This outcome underlines the potential of distributed models to offer reliable inference capabilities that match those of traditional centralized models, without sacrificing predictive accuracy. Furthermore, the adoption of distributed models enhances privacy for data owners, presenting a compelling case for their broader application in data-sensitive environments. Overall, our results suggest that it is possible to implement a distributed model without significantly losing information. Our analysis suggests that SGD, Adaboost and Naive Bayes approaches are suitable for such distributed approached with tabular data. However, MLPerceptron, Decision Trees and \ac{knn} don't seem to be a good approach for such use cases.

However, there are still issues to be addressed. This methodology presents hurdles regarding categorical class handling. Firstly, all classes should be known first-hand and should be given to each model even if that silo in particular has no cases of that class. Secondly, low-frequency classes are also an issue to be addressed, since training the model with cross-validation will raise problems because each split should have all classes present. Our approach relied on sample creation for low and non-existent target classes. However, this approach is adding information to the model that is not originally there. The way we chose for minimising this issue was by creating dummy variables with median and mode imputations based only on the information in the dataset. Nevertheless, non-existent classes are impossible to address without prior information. These class problems could be partially tackled in production by implementing data management and governance procedures, namely data dictionaries. Still on data preprocessing, we applied ordinal encoding to the variables which will create a natural hierarchy between variables. One solution for this is to create binary columns for each class in each column. This will remove the hierarchy between classes but increase variable numbers and training time considerably.\\
%Moreover, like in most secondary usage of data, other issues are important to keep in mind, even in such a controlled environment as this one. Even though the software is the same in every hospital, the clinical service is the same and the underlying data models are the same, the version of the software is not the same across all hospitals. This difference alone can alter the way each column is populated, mainly through front-end changes or label modification, among other aspects. Additionally, each hospital has its own workflows in practice that can also alter the way data is collected; changing timings or steps in a certain workflow can dramatically change the data acquisition and the reality it represents. \\
Another issue to consider is the path adopted to build the distributed model. In this case, it was decided to develop an ensemble of models with voting. However, other methods could have been employed, like parameter averaging, that should be tested as well. In particular, the usage of more robust neural networks could be assessed as well. We chose not to test state-of-the-art neural networks since the data volume was low for that use case and several papers have already demonstrated that neural networks are not the most suitable tool for tabular data \cite{grinsztajnWhyTreebasedModels2022,borisovDeepNeuralNetworks2022}. We chose to add MLPerceptron as a baseline for comparison with the remaining algorithms. The results show us that the performance was below the other algorithms, but in this concrete case, the problem may reside in the architecture chosen and hyperparameters used in the Cross-validation which may have lead to underfitting. Despite this, a precise and thorough demonstration of this use case would be important to consider such scenarios. \\
Furthermore, the algorithm underlying the distributed model is of importance as well for its performance versus the centralised model. Figures \ref{fig:heatmap-cat} and \ref{fig:heatmpa-int} and table \ref{tab:hyp} show us that Decision trees and \ac{knn} implemented in a centralised manner are consistently better than the distributed counterpart. This is specially notorious in the case of the decision trees. We believe this may be related to way the algorithm is implemented. A centralised version may be able to create optimal splits in the data, while the distributed version may not be able to do so. This is a topic that should be further explored.\\
Even though this improvement may have a relationship to the target variable (i.e. figure \ref{fig:heatmpa-int} for IA and IGA variables), it is still an important fact to take into account when implementing such architectures.
The performance of the models is also interesting to catch differences in silos. See silo 6 for TPNP (figure \ref{fig:heatmap-cat}) where silo 6 consistently behaves differently than the rest.
Checking performance data regarding regression tasks, we can see a drop in performance for PI and IA. While the explanation for the performance of IA can be explained by the average value of it which is 66. This is the highest average in the dataset. This means that the model will have a harder time predicting these values. This is also true for the distributed model. This is a topic that should be further explored.\\
As for implementation, such a mechanism could be implemented in at least two manners; with a central orchestrator or without. The first one would assume a central point that would make a request to each silo for a prediction and then create the final prediction with the weighted averaging of each one. The second one would not require any additional platform and each silo would communicate with each of the others and receive the prediction and would create the final with their own. This implementation step would of course take into account variables that we were out of scope such as the communication between silos. 
Regarding the prediction capability as a whole, we found that this data is suitable to apply machine-learning models in order to predict several clinical outcomes, with very good results for several target variables. 

%Another topic that can be explored is the fact that a lower performance of the ensemble in a certain silo and/or target could be actually useful to characterise the population of that silo.
