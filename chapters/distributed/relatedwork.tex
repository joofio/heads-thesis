% !TeX root = ../../thesis.tex

Distributed learning \cite{distributed} can be understood as training several models in a different setting and then aggregating them as a whole. There are two main branches of these approaches, distinguishable by the existence of a central orchestrator server: federated learning where such an entity exists, and peer-to-peer (or swarm) \cite{swarm_learning} learning where it does not. \\
Even though distributed learning has been receiving a lot of attention recently, only some of its concepts have been focused on, mainly distributed-deep learning with a federated learning approach \cite{xuFederatedLearningHealthcare2021,leeFederatedLearningClinical2020}. These methods use the strength of neural networks and several algorithms such as federated averaging to create distributed models capable of handling complex data like text, sound, or image \cite{prayitnoSystematicReviewFederated2021}. However, considering that there are great amounts of information, especially in healthcare, stored as tabular data \cite{alvarezsanchezTAQIHToolTabular2019,dimartinoExplainableAIClinical2022,payrovnaziriExplainableArtificialIntelligence2020,mcelfreshWhenNeuralNets2023,klambauerSelfNormalizingNeuralNetworks2017} and that neural networks are often not the best tool for such data structures and often outperformed by boosting algorithms and tree based models \cite{borisovDeepNeuralNetworks2022a,grinsztajnWhyTreebasedModels2022a}, there is a lack of knowledge in the traditional \ac{ml} techniques in a distributed manner. This is specially important since tabular data comes mainly from \acp{ehr} and this kind of data is often of lower quality, with missing values, and with a high number of categorical variables and unstructured/semi-structured variables which make the application of classical machine-learning algorithms harder than for example images, which are mainly computer and systematically generated \cite{peekThreeControversiesHealth2018}.
%Federated Learning was introduced in 2016 \cite{konecny_federated_2016,mcmahanFederatedLearningDeep2016} and it was called federated since "the learning task is solved by a loose federation of participating devices (which we refer to as clients) which are coordinated by a central server" \cite{konecny_federated_2016}. Federated learning has two main architectures: a) horizontal and b) vertical  \cite{yangFederatedMachineLearning2019b}. 
%Horizontal refers to having the same features in all silos, but different populations in each silo. Vertical refers to having different features across silos for the same population. Then we have other approaches that expand the concept of federated learning with previous machine learning and deep learning methodologies such as transfer learning, reinforcement learning \cite{liuSystematicLiteratureReview2020} and quantum machine-learning \cite{quantum-fed-ml}. 
%Federated learning can also be classified by the information flow. Model data can be shared only with the main central server, as in more traditional methods, but also being incremental, sharing data model sequentially between silos, with central server orchestration \cite{cyclic_distribution}. 

Nevertheless, there have been some health-related distributed machine-learning projects successfully implemented, such as euroCAT \cite{eurocat} which implemented an infrastructure across five clinics in three countries. SVM models were used to learn from the data distributed across the five clinics. Each clinic has a connector to the outside where only the model's parameters are passed to the central server which acts as a master deployer regarding the model training with the radiation oncology data. Also, ukCAT \cite{ukcat} did similar work, with an added centralised database in the middle, but the training being done with a decentralised system. There are also reports of a study that introduces "confederated machine learning" for modeling health insurance data that is fragmented both horizontally (by individual) and vertically (by data type), without the need for central data consolidation. It showcases the method's efficacy in predicting diseases like diabetes and heart conditions across data silos, achieving notable prediction accuracy, thereby advancing federated learning in healthcare by accommodating complex data separations and enhancing model training without compromising patient privacy or data security \cite{liuConfederatedLearningHealthcare2022}. Distributed initiatives have also been covered in a review by Kirienko et al., \cite{kirienkoDistributedLearningReliable2021a}, where we can see very few papers have described a distributed learning approach without federation. However, from these, we can highlight the works of Wang et al. \cite{wangFastDivideandconquerSparse2019} tried to use these approaches to detect re-hospitalization for heart failure and Tuladhar et al., \cite{distributed} where they used the distributed approach to detect several diseases like diabetes, heart disease, and mild cognitive impairment.

Finally, a few works have explored the evaluation of models in a distributed manner, for example comparing  centralised \ac{ml} and distributed \ac{ml} on MNIST dataset \cite{performance_evaluation_1}. Also, works that evaluate federated learning on MNIST, MIMIC-III and PhysioNet ECG datasets, but not in comparison with other methods \cite{performance_evaluation_2}. The work by Tuladhar and colleagues \cite{distributed} uses healthcare images and/or public and curated datasets. Furthermore, these findings are supported by a scoping review which clearly states that proper  evaluation of distributed/federated when compared to local methodologies and models \cite{liFederatedDistributedLearning2023}. With this, as far as we know, this is the first time that an evaluation of distributed \ac{ml} has been conducted using real-world tabular clinical data from several real (9) different sources, in such a large scale of algorithms and outcome variables and compared to centralised and local counterparts, which can be applied to both federated and peer-to-peer approaches.