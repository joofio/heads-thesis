\begin{savequote}[85mm]
Nothing great in the world was accomplished 
without passion.
\qauthor{Friedrich Hegel}
\end{savequote}

\chapter{Case Studies}\label{chap:usecase}
This chapter will comprise the work done during this PhD. The works developed and corresponding papers were a search for improving data usage in several steps of the \ac{kdd} process. We can see some work dedicated to leveraging data acquisition or alternatives to it, like the works depicted in section \ref{subsec:distributed}, \ref{subsec:benchmark}, \ref{subsec:similarity}, \ref{subsec:tabular} and \ref{subsec:gans}. Others will focus more on how to use the data in order to make a difference in clinical practice like the section \ref{subsec:ipop}, \ref{subsec:obs} and \ref{subsec:dq}.



\section{Can GANs help create realistic datasets?}\label{subsec:gans}
This section is based on the paper entitled "GANs for Tabular Healthcare Data Generation: A Review on Utility and Privacy". It focuses on a review of the \ac{gan} framework for creating synthetic data for healthcare. Tries to compile the metrics used for comparing and assessing synthetic data in terms of utility - or how similar they are to the original data and privacy - how protective of the patient's data it is. 

\input{chapters/gans-paper}

\section{Pulling the current metrics of assessing datasets}\label{subsec:tabular}
This section is based on the paper entitled "Dataset Comparison Tool: Utility and Privacy". This work followed the work on section \ref{subsec:gans}, where we compiled ways of assessing the utility of synthetic data. We understood that the mechanisms were far from consensual and a tool could be of use to merge all of this into a single file and report about data. Our purpose was to facilitate health data owners and legal responsible to understand how similar and protective a dataset was regarding the original one.

\input{chapters/tabular-paper}



\section{Can we use machine learning feature to compare datasets?}\label{subsec:similarity}
This section is based on the paper entitled "Using Machine Learning Models' feature importance to assess dataset similarity". The reasoning behind this paper was the results of \ref*{subsec:gans}, where we felt that evaluation metrics for synthetic data could be improved. Better yet, we felt that the comparison of two datasets (that shared the same columns) could be done in a more robust way. Being that the current gold-standard was cross-validation which was not bound to any number range and the significance of the result could not be easily interpretable. We used the feature importance of several \ac{ml} models to compare datasets and concluded that it was a valid alternative to the traditional metrics.

\subsection{Introduction}
\input{chapters/similarity/intro}
\subsection{Rationale and Related Work}
\input{chapters/similarity/relatedwork}
\subsection{Materials \& Methods}
\subsubsection{Method Overview}
\input{chapters/similarity/methods}
\subsubsection{Data used}
\input{chapters/similarity/materials}
\subsection{Results}
\input{chapters/similarity/results}
\subsection{Discussion}
\input{chapters/similarity/discussion}
\subsection{Conclusion}
\input{chapters/similarity/conclusion}


\section{Data quality Metrics}\label{subsec:dq}
This section is based on the paper entitled "Development and Validation of a Data Quality Evaluation Tool in Obstetrics Real-World Data through HL7-FHIR interoperable Bayesian Networks and Expert Rules" This paper focuses on the fact that data quality is a major concern in healthcare. We developed a tool that could be used to assess the quality of data in a \ac{ehr} and provide a report on the quality of the data. We used a combination of \ac{bn} and expert rules to assess the quality of the data. Furthermore, we tested the tool on 9 real-world datasets of obstetrics \acp{ehr} and concluded that the tool was a valid alternative to the traditional methods of assessing data quality.
%\input{chapters/data-quality-paper}
\subsection{Introduction}
\input{chapters/data-quality/intro}
\subsection{Background and Related Work}
\input{chapters/data-quality/relatedwork}
\subsection{Materials}
\input{chapters/data-quality/materials}
\subsection{Methods}
\input{chapters/data-quality/methods}
\subsection{Results}
\input{chapters/data-quality/results}
\subsection{Discussion}
\input{chapters/data-quality/discussion}
\subsection{Conclusion}
\input{chapters/data-quality/conclusion}




\section{Leveraging Distributed systems in healthcare: is it advisable?}\label{subsec:distributed}
This section is based on the paper entitled "Evaluating distributed-learning algorithms on real-world healthcare data". This paper was focused on the fact that access to healthcare data is often laboursome and time-consuming. So we evaluated the distributed paradigm to its gold-standard, the centralized paradigm. We used 9 real-world datasets of obstetrics \acp{ehr} and compared the performance of several \ac{ml} algorithms in both paradigms. We concluded that the distributed paradigm is a valid alternative to the centralized paradigm, with the added benefit of not requiring heavy data sharing.

\subsection{Introduction}
\input{chapters/distributed/intro}
\subsection{Theoretical background and Related Work}
\input{chapters/distributed/relatedwork}
\subsection{Materials}
\input{chapters/distributed/materials}
\subsection{Methods}
\input{chapters/distributed/methods}
\subsection{Results}
\input{chapters/distributed/results}
\subsection{Discussion}
\input{chapters/distributed/discussion}
\subsection{Conclusion}
\input{chapters/distributed/conclusion}


\section{Can Institutions share their performance metrics without hesitation of retaliation?}\label{subsec:benchmark}
This section is based on the paper entitled "Benchmarking institutions' health outcomes with clustering methods". This paper was focused on the fact that many healthcare institutions harbor reservations about openly sharing production metrics. One predominant concern is the potential for retaliatory actions, be it from regulatory bodies, competitors, or the public. In this paper, we propose the application of a clustering methodology that allows institutions to compare performance metrics without disclosing the actual values. The method is based on clustering, which involves grouping health institutions' outcomes into a known number of clusters, allowing institutions to position themselves in a range of clusters without sharing the true means of their target data. The proposed method uses the K-means and K-modes clustering algorithms and was tested on data from real Electronic health records and public datasets. This approach provides a valid benchmark of hospital metrics and performances while protecting the privacy of participating institutions. 
\subsection{Introduction}
\input{chapters/benchmark/intro}
\subsection{Rationale and Related Work}
\input{chapters/benchmark/relatedwork}
\subsection{Materials \& Methods}
\subsubsection{Method Overview}
\input{chapters/benchmark/methods}
\subsubsection{Data used}
\input{chapters/benchmark/materials}
\subsection{Results}
\input{chapters/benchmark/results}
\subsection{Discussion}
\input{chapters/benchmark/discussion}
\subsection{Conclusion}
\input{chapters/benchmark/conclusion}




\section{Leveraging data to assess treatment efficacy}\label{subsec:ipop}
This section is based on the paper entitled "Comparative Analysis of Palbociclib and Ribociclib: A real world data and Propensity Score-Adjusted Evaluation with endocrine therapy". This was a method of applying the knowledge of causality and transparent \ac{ml} models in order to assess the real-world effect of two drugs for breast cancer. We started with traditional analysis and then moved to a more complex approach, using \ac{iptw} methods in order to further compare treatments.


\subsection{Introduction}
\input{chapters/ribo-palbo/intro.tex}
\subsection{Materials \& Methods}
\input{chapters/ribo-palbo/mat_methods.tex}
\subsection{Results}
\input{chapters/ribo-palbo/results.tex}
\subsection{Discussion}
\input{chapters/ribo-palbo/discussion.tex}
\subsection{Conclusion}
\input{chapters/ribo-palbo/conclusion.tex}


\section{Leveraging data to create Clinical Decision Support Systems}\label{subsec:obs}
This section is based on the paper entitled "Machine-learning in Obstetrics: FHIR-based Support System for predicting delivery type". This work was in part a result of the work in section \ref{subsec:distributed}. While testing for distributed mechanisms, we kind of felt that some evaluation metrics were inspiring to pursue this further. We built a \ac{cdss} system that is interoperable and aims to provide support for subpar evaluation of a \ac{cs}.

\subsection{Introduction}
\input{chapters/obs/intro}
\subsection{Rationale and Related Work}
\input{chapters/obs/relatedwork}
\subsection{Materials}
\input{chapters/obs/materials}
\subsection{Methods}
\input{chapters/obs/methods}
\subsection{Results}
\input{chapters/obs/results}
\subsection{Discussion}
\input{chapters/obs/discussion}
\subsection{Conclusion}
\input{chapters/obs/conclusion}





