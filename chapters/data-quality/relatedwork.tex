There is already a significant number of papers trying to define data quality assessment frameworks for \ac{ehr} data, all of them plausible and recommendable, already described in other papers \cite{bianAssessingPracticeData2020}. The literature has over 20 different methods, descriptions, and summaries of  different frameworks over the years. Some may be highlighted from the review from Weiskopf et al., \cite{weiskopfMethodsDimensionsElectronic2013}, where five data quality concepts were identified over 230 papers: Completeness, Correctness, Concordance, Plausibility and Currency. 



The work of Saez et al. defined a unified set of \ac{dq} dimensions: completeness, consistency, duplicity, correctness, timeliness, spatial stability, contextualization, predictive value, and reliability \cite{saezOrganizingDataQuality2012}. Then a review of Bian et al. \cite{bianAssessingPracticeData2020} expanded on the previous ones, categorizing data quality into 14 dimensions and mapping them to the previous most known definitions. These were: currency, correctness, plausibility, completeness, concordance, comparability, conformance, flexibility, relevance, usability, security, information loss, consistency, and interpretability.

Finally, the work of Khan et al. tried to harmonize data quality assessment frameworks, which simplified all previous concepts into three main categories: Conformance, Completeness and Plausibility and two assessment contexts: Verification and Validation \cite{kahnHarmonizedDataQuality2016a}.
Despite all of these comprehensive works, there is still no consensus regarding which one is best or which has taken the lead in usage. Moreover, looking at all of the descriptions related in the literature, a significant portion of concepts are overlapping, and sometimes hard to conceptualize such dimensions in practice.

As for implementations, there are already some available, such as the work from \cite{phanAutomatedDataCleaning2020} where a tool created by primary care in the Flanders was built to assess completeness and percentage of values within the normal range.
The work from Liaw et al. \cite{liawQualityAssessmentRealworld2021} already reviewed some data quality assessment tools, like tools from OHDSI \cite{hripcsakObservationalHealthData2015} or TAQIH \cite{alvarezsanchezTAQIHToolTabular2019}. 
Additionally, we found some others with similar purposes and characteristics like the work presented data dataquieR \cite{schmidtFacilitatingHarmonizedData2021}, an R language-based package that can assess several data quality dimensions in observational health research data. 
Also, the work from Razzaghi et al. developed a methodology for assessing data quality in clinical data \cite{razzaghiDevelopingSystematicApproach2022}, taking into account the semantics of data and their meanings within their context. Furthermore, the work from Rajan et al. \cite{rajanContentAgnosticComputable2019} presented a tool that can assess data quality and characterize health data repositories. Parallel to this, Kaspner et al. created a tool called DQAStats that enables the profiling and quality assessment of the MIRACUM database, being possible to integrate into other databases as well \cite{kapsnerLinkingConsortiumWideData2021a}.

Regarding data quality assessment as a whole, the works of \cite{estiriSemisupervisedEncodingOutlier2019}, focused on outlier detection in large-scale data repositories. The works of \cite{saezEHRtemporalVariabilityDelineatingTemporal2020} focused on the exploration and identification of dataset shifts, contributing to the broad examination and repurposing of large, longitudinal data sets. The works of García-de-Léon-Chocano \cite{saStandardizedDataQuality2017,garcia-de-leon-chocanoConstructionQualityassuredInfant2016,garci;a-de-leon-chocanoConstructionQualityassuredInfant2015} are the only ones focused on obstetrics data, but aimed to improve the process of generating high quality data repositories for research and best practices monitoring. These are similar and complementary works to this one. Finally, the work of \cite{springateREHRPackageManipulating2017} focused on the manipulation of \ac{ehr} data, including data quality assessment, data cleaning, and data extraction. However, these tools are not meant to be used at the production level, assessing data as it is being registered or outputs reports for human consumption and not a quantitative metric for metric comparison. Furthermore, none of these tools had standard-based interoperability in mind. Finally, we have not seen, until the moment of this paper, any implementation that used machine learning to evaluate the correctness of the value.
