There is already a significant number of papers trying to define data quality assessment frameworks for EHR data, all of them plausible and recommendable, already described in other papers \cite{bianAssessingPracticeData2020}. The literature has over 20 different methods, descriptions, and summaries of  different frameworks over the years. Some may be highlighted from the review from Weiskopf et. al, \cite{weiskopfMethodsDimensionsElectronic2013}, where five data quality concepts were identified over 230 papers: Completeness, Correctness, Concordance, Plausibility and Currency. 
Then Khan et al. tried to harmonize data quality assessment frameworks, which simplified all previous concepts into three main categories: Conformance, Completeness and Plausibility and two assessment contexts: Verification and Validation \cite{kahnHarmonizedDataQuality2016a}.
Then a review of Bian et al.  \cite{bianAssessingPracticeData2020} expanded on the previous ones, categorizing data quality into 14 dimensions and mapping them to the previous most known definitions. These were: currency, correctness, plausibility, completeness, concordance, comparability, conformance, flexibility, relevance, usability, security, information loss, consistency and interpretability. 
Despite all of these comprehensive works, there is still  no consensus regarding which one is best or which has taken the lead in usage. Moreover, looking at all of the descriptions related in the literature, a significant portion of concepts are overlapping and sometimes hard to conceptualize such dimensions in practice. 

As for implementations, there are already some available, such as the work from \cite{phanAutomatedDataCleaning2020} where a tool created by primary care in the Flanders was built to assess completeness and percentage of values within the normal range.
The work from Liaw et al. \cite{liawQualityAssessmentRealworld2021} already reviewed some data quality assessment tools, like tools from OHDSI \cite{hripcsakObservationalHealthData2015} or TAQIH \cite{alvarezsanchezTAQIHToolTabular2019}. 
Additionally, we found some others with similar purposes and characteristics like the work presented data dataquieR \cite{schmidtFacilitatingHarmonizedData2021}, an R language-based package that can assess several data quality dimensions in observational health research data. 
Also, the work from Razzaghi et al. developed a methodology for assessing data quality in clinical data \cite{razzaghiDevelopingSystematicApproach2022}, taking into account the semantics of data and their meanings within their context. Furthermore, the work from Rajan et al. \cite{rajanContentAgnosticComputable2019} presented a tool that can assess data quality and characterize health data repositories. Parallel to this, Kaspner et al. created a tool called DQAStats that enables the profiling and quality assessment of the MIRACUM database, being possible to integrate into other databases as well \cite{kapsnerLinkingConsortiumWideData2021a}.
However, these tools are not meant to be used at the production level, assessing data as it is being registered or outputs reports for human consumption and not a quantitive metric for metric comparison. Furthermore, none of the non-agnostic tools were designed for obstetric EHR data. Finally, we have not seen, until the moment of this paper, any implementation that used machine learning to evaluate the correctness of the value.

