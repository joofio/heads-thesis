% !TeX root = ../../thesis.tex

There is already a significant number of papers trying to define data quality assessment frameworks for EHR data, all of them plausible and recommendable, already described in other papers \cite{bianAssessingPracticeData2020}. The literature has over 20 different methods, descriptions, and summaries of different frameworks over the years. Some may be highlighted from the review from Weiskopf et al., \cite{weiskopfMethodsDimensionsElectronic2013}, where five data quality concepts were identified over 230 papers: Completeness, Correctness, Concordance, Plausibility, and Currency. 
The work of Saez et al. defined a unified set of DQ dimensions: completeness, consistency, duplicity, correctness, timeliness, spatial stability, contextualization, predictive value, and reliability \cite{saezOrganizingDataQuality2012}. Then a review of Bian et al. \cite{bianAssessingPracticeData2020} expanded on the previous ones, categorizing data quality into 14 dimensions and mapping them to the previous most known definitions. These were: currency, correctness, plausibility, completeness, concordance, comparability, conformance, flexibility, relevance, usability, security, information loss, consistency, and interpretability.

Finally, the work of Khan et al. tried to harmonize data quality assessment frameworks, which simplified all previous concepts into three main categories: Conformance, Completeness, and Plausibility, and two assessment contexts: Verification and Validation \cite{kahnHarmonizedDataQuality2016a}. Conformance  assesses if data values adhere to specified standards and formats. For instance, checking if a data field like 'gender' conforms to accepted values such as 'M', 'F', or 'U'. Completeness focuses on whether all necessary data values are present. An example would be checking for missing values in a critical data field like 'patient ID'. Plausibility evaluates the believability or truthfulness of data values. An example is verifying that the dates in a dataset (like birth date and date of diagnosis) follow a logical order, where the birth date precedes the diagnosis date. Despite all of these comprehensive works, there is still no consensus regarding which one is best or which has taken the lead in usage. Moreover, looking at all of the descriptions related in the literature, a significant portion of concepts are overlapping, and sometimes hard to conceptualize such dimensions in practice.


As for implementations, there are already some available, such as the work from \cite{phanAutomatedDataCleaning2020} where a tool created by primary care in the Flanders was built to assess completeness and percentage of values within the normal range. The work from Liaw et al. \cite{liawQualityAssessmentRealworld2021} already reviewed some data quality assessment tools, like tools from OHDSI \unskip~\cite{hripcsakObservationalHealthData2015} or TAQIH \unskip~\cite{alvarezsanchezTAQIHToolTabular2019}. Additionally, we found some others with similar purposes and characteristics like the work presented data dataquieR \unskip~\cite{schmidtFacilitatingHarmonizedData2021}, an R language-based package that can assess several data quality dimensions in observational health research data. Also, the work from Razzaghi et al. developed a methodology for assessing data quality in clinical data \unskip~\cite{razzaghiDevelopingSystematicApproach2022}, taking into account the semantics of data and their meanings within their context. Furthermore, the work from Rajan et al.\unskip~\cite{rajanContentAgnosticComputable2019} presented a tool that can assess data quality and characterize health data repositories. Parallel to this, Kaspner et al. created a tool called DQAStats that enables the profiling and quality assessment of the MIRACUM database, being possible to integrate into other databases as well\unskip~\cite{kapsnerLinkingConsortiumWideData2021a}.


Regarding data quality assessment as a whole, the works of\unskip~\cite{estiriSemisupervisedEncodingOutlier2019}, focused on outlier detection in large-scale data repositories. The works of\unskip~\cite{saezEHRtemporalVariabilityDelineatingTemporal2020} focused on the exploration and identification of dataset shifts, contributing to the broad examination and repurposing of large, longitudinal data sets. The works of Garc\'{\i}a-de-L{\'e}on-Chocano\unskip~\cite{garci;a-de-leon-chocanoConstructionQualityassuredInfant2015,garcia-de-leon-chocanoConstructionQualityassuredInfant2016,saStandardizedDataQuality2017} are the only ones focused on obstetrics data, but aimed to improve the process of generating high quality data repositories for research and best practices monitoring. These are similar and complementary works to this one. Finally, the work of\unskip~\cite{springateREHRPackageManipulating2017} focused on the manipulation of EHR data, including data quality assessment, data cleaning, and data extraction. However, these tools are not meant to be used at the production level, assessing data as it is being registered or outputs reports for human consumption and not a quantitative metric for metric comparison. Furthermore, none of these tools had interoperability in mind. Finally, we have not seen, until the moment of this paper, any implementation that used \ac{ml} to evaluate the correctness of the value.

