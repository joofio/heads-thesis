This work adds several pieces of information to the state of the art of data quality analysis. First we tried to map the output of a automatic assessment tool to the human perception of quality and the issues linked to doing so. Secondly, the fact that we applied explainable machine learning methods such as bayesian networks to leverage the potency of advanced data anlysis without compromising interperability and explainability. Furthermore, a single model was able to reach high performance metrics for almost all variables. Thirdly, the fact that interoperability standard such as FHIR can be adopted to facilitate the usage and information exchange of such tools. However, there are also shortcoming and challenges to address. The first is that data quality is still an elusive concept since it has a contextual dimension and the quality of the record depends on the usage of the information. For example, data aimed at primary usage and day-to-day healthcare decisions about a patient will have different requirements regarding the importance of some variable or completeness of information very different from data needed to create summary statistics for key performance indicators extraction. Moreover, the data is still very vendor-specific. Even though we used an interoperability standard, the semantic layer, more connected with terminology is still lacking. This is an issue to be addressed in order to improve the interoperability of the standard. Moreover, we do not know how the training done with this data is generalizable to other vendors. One opportunity arises of mapping all of this data to a widely used terminology like SNOMED CT or LOINC. Nevertheless, the usage of FHIR and the fact that the data is mapped to a standard terminology, makes it easier to use the data in other systems and to compare the results with other studies. Furthermore, being available freely and online makes it easier to understand how to map vendor-specific datasets to the model and use it in other contexts. Regarding the model, the usage of explainable methodologies like outlier-tree and transparent models like Bayesian networks are vital for clinical application. Since we use a single model to classify possible errors in the records, the ability to try to show clinicians why that value was tagged is of uttermost importance in order to get feedback and action from humans. From the experience gathered with the study, we believe that a weaker but transparent model could have more impact than better performant but opaque ones. If explainability and interpretability are important for any ML problem, this need only increases when we are dealing with such subjective concepts as data quality.

Regarding the clinical evaluation, we found out that asking clinicians to purely assess the quality of a record in an EHR is not an easy task. We found out that for a proper assessment, a context and objective must be defined in order to make the evaluation more objective. Moreover, the ranking methodology, even though is very useful for comparing with the model, it is not easy for clinicians to order 10 records when some of them have the ”same level” of quality. This is a very important aspect to take into account when designing the evaluation of data quality. Probably a categorical evaluation of yes/no could be more useful and then compare it with the ordering of the model and define thresholds based on that. These reasons are probably the cause behind the great variability between clinicians and between clinicians and the model. However, we do see a tendency for a higher agreement of the worst quality results than the best ones. This result suggests that the system may not be suitable for ranking good-quality records and clinicians are also not able. However, it could be useful to alert for low-quality ones, which is also a very important task with a great impact on the quality of the data. These findings are supported not only by figure \ref{fig:clinical} but also by figure \ref{fig:scoring} where we can add some threshold for the need for a human review. From the preliminary data in the questionnaires and looking at the graph, we believe that a threshold of around 0.3 could be a good starting point. However, this is a very subjective decision, and it should take into account the context and the objective of the evaluation. For example, if the objective is to use the data for research, a higher threshold could be used. On the other hand, if the objective is to use the data for day-to-day clinical decisions, a lower threshold could be used.
For the next steps, we believe a research path could be of identifying contexts for applying data quality checks like primary usage, research purposes, and aggregated analysis for decision-making among others could help better target those contexts and the importance of each variable for those use cases. This could be interesting to add to the tool in order to weigh the different variables according to the context.


%discutir que dependendendo da pessoa /context/profissao/objecti pode ser preciso dar pesos diferentes a diferentes colunas.
%para dar GDH nao ha coisas que sao precisas mas ha coisas vitais
%para financeiro ha so umas precisas (tipo meds e procs)
%para clinico ha outras e por ai fora