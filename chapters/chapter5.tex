\begin{savequote}[75mm]
We never are definitely right, we can only be sure we are wrong.
\qauthor{Richard P. Feynman}
\end{savequote}
\chapter{Discussion} \label{chap:disc}

%frases mt curtas

Extracting knowledge from healthcare data is not easy. It relies on the availability of data, which is not always the case, and on the ability to extract knowledge from it. In this chapter, we discuss the main challenges we faced during the development of this thesis, and how we overcame them. We also discuss the limitations of our work, and how it can be improved in the future. Finally, we discuss the main contributions of this thesis, and how they can be used to improve the quality of healthcare.
The first problem is getting access to data. The data is not always available, and when it is, it is not always in the format we need. Ethics committees and \ac{dpo} requirements are put in place in order to guarantee the patient's privacy and security, but a lot of times at the cost of timely access to data. I consider that synthetic data can have a good impact on this work. While we can leave the legal processes be, we may use synthetic data with a heavy focus on security to develop and test our algorithms. This is a very promising area of research, and I believe it will be a game-changer in the future.
Parallel to this approach are distributed paradigms. Having a distributed approach to data analysis could be of great help. This would allow for the data to be analyzed in its original location in a more secure way and timely manner. If metrics and models could be built by local teams and shared across regions and/or countries to leverage the power of the many for single institutions could be groundbreaking. However, underlying both these approaches are data dictionaries and data governance tools. Having the correct functional/clinical description of data could be of great impact on the usage of data. Having already the variables defined as categorical, numerical and so on could be of great help. This is a very important aspect of data science, and it is often overlooked. Simple statistics of datasets could be useful as well. For example, the number of missing values, the number of unique values, the number of outliers, and so on. This would help the data scientist to understand the data better and to know what to expect from it. 

This issue also relates to the second big hurdle of knowledge extraction from healthcare data - quality. As discussed in section \ref{subsec:dq}, this is a very complex and sometimes elusive concept. In our case, this implied a lot of time spent with data preprocessing. We had to deal with missing values, outliers, and correctness in the context of the records, and data in different formats. We also had to link together different databases from different \acp{his} which brought to light new problems like the new dimensions of correctness of data. There is a common saying that sums this pretty well \textit{When we have one watch, we know the time, but when we have two, we may never know}. So if we had different information regarding the same variable in different systems, how to decide what is true?
Another aspect that is often overlooked is the relationship with the clinicians. We need to understand that they are the ones who will use the tools we develop, and they need to be involved in the process. We need to understand their needs and their workflow. We need to understand what they need and how they need it. We need to understand that they are not data scientists, and they do not have the time to learn how to use our tools. We need to make it easy for them to use our tools. Now healthcare is often explained in terms of clinical teams of different backgrounds. A similar concept could be beneficial for harvesting knowledge from data.
Thirdly, building software or tools based on this data is still an early subject that possibly requires a legal and technical framework. A legal is connected to the impact of such tools in healthcare. If drugs require such a long time to be approved in order to assess security, how can we approve a tool that can have a similar impact? A technical framework is connected to the fact that we are still in the early stages of a new health data science paradigm. We are still trying to understand how to use data, and how to extract knowledge from it. We are still trying to understand how to evaluate the performance of our tools.  We are still trying to understand how to evaluate the impact of our tools in healthcare in a timely manner in a way that is not biased and that is not too expensive. Imposing similar structures to drugs is ill-advised since it could possibly kill the innovation potential and the interest in providing such tools. And this is where a quality infrastructure could be of use. Seriously betting of biomedical informatics could render huge payoffs down the line. Having the human and material resources to build data infrastructures on local (healthcare institutions) and regional, or even country-wise or cross-country policies to use effective use healthcare data is essential. At the time of the writing of this thesis, examples like \ac{ehds} are very promising initiatives that could help to overcome the hurdles of data availability and quality. However cross-country initiatives will always be as good as the weakest link, so it is important to have a common framework and a common goal and to have the resources to achieve it. In concrete, having data pipelines, data governance and data interoperability tools, and data quality tools are essential. Having a common data dictionary and a common data format would also be of great help. This would allow for a more efficient use of data, and it would allow for the use of healthcare data to drive innovation.
Tightly connected with this is the possibility of having \ac{rwe} support clinical decisions live. Having data like the one produced in \ref{subsec:ipop} in real-time or with high update frequency could be leveraged in order to further support clinicians in making decisions based on data. However, we would require not only the premisses already discussed, like data quality and cross-colaboration clinics, but a trust-framework would also be necessary. In order to make the automatic dashboard and metrics reliable, transparency is key. Having explainability and transparency in the process of evidence production will be key to building trust and accountability.

%data quality é um sério problema - buscar papers ricardo e isso
%data dictionary é vital
%relacao com medicos é vital
%synthetic data é mirage - mas pode ser util por contexto - IPOP
%Metricas de avaliação sao fundamentais, nao conseguimos gerir o que nao conseguimos medir
%focar no impacto
%usar métodos alternativos - distribuido, p.ex.
%metodos de analise em tempo real de eficácia medicamentos - mt trabalho manual
%data engineering, preprocessing - é super dificil e de facto demora muito mais tempo.



The challenges of extracting knowledge from healthcare data are multi-faceted, as evident from the issues of data access, quality, and the complex relationship with clinicians. Another vital aspect is the integration of real-world evidence (RWE) into clinical decision-making processes. RWE, derived from data collected outside of controlled clinical trials, offers immense potential for informing healthcare decisions. However, its integration requires meticulous attention to data quality, governance, and transparency. As healthcare data becomes increasingly digitized and voluminous, the opportunity to leverage RWE in real-time or with high-frequency updates grows. This could significantly enhance the ability of clinicians to make data-driven decisions. However, for RWE to be effectively integrated, it necessitates not only robust data infrastructure but also a trust framework. Clinicians and patients alike must have confidence in the accuracy, reliability, and transparency of the data and the algorithms used. Building this trust involves ensuring that data processing and decision-making algorithms are transparent and explainable, fostering a sense of accountability and reliability in the system.

Furthermore, the evolution of healthcare data science underscores the need for a comprehensive legal and technical framework. The comparison to drug approval processes highlights the importance of stringent evaluation for healthcare tools, balancing safety and innovation. The legal framework should address the ethical implications and societal impact of these tools, while the technical framework should focus on performance evaluation, data extraction techniques, and impact assessment. Establishing such frameworks is crucial for navigating the complexities of health data science and for fostering an environment where innovation can thrive without compromising patient safety or data integrity. This approach also involves the creation of quality infrastructures, emphasizing biomedical informatics, and developing robust data infrastructures at various levels, from local healthcare institutions to regional and international collaborations.

Lastly, the future of healthcare data science depends heavily on cross-disciplinary collaboration and common frameworks. Initiatives like the European Health Data Space (EHDS) are steps in the right direction, promoting data availability and quality through collaborative efforts. However, the success of such initiatives relies on the strength of their weakest links, necessitating uniform standards, shared goals, and adequate resources across all participating entities. Concrete measures like establishing common data dictionaries, data formats, and interoperability tools are essential. These efforts will pave the way for more efficient data utilization, driving innovation in healthcare. Such an integrated approach, combining technical prowess with legal and ethical considerations, is vital for realizing the full potential of healthcare data in improving patient outcomes and advancing medical science.