% !TeX root = ../../thesis.tex


With the results obtained, we believe that there are better alternatives to \ac{cc} (RS/SR). \ac{rbo} seems to be a viable alternative. Firstly, as with Kendall tau and Weighted Kendall tau, it appears to be directly linked to differences in the dataset. The metric tends to decrease from 1 (identical datasets) to approximately 50\% when half of the dataset's columns are altered, which is an encouraging outcome. Secondly, it is a bounded metric between 0 and 1, which aids interpretation. This is particularly useful, as metrics like \ac{cc} and $R^2$ can fall outside these bounds, complicating the interpretation of results in some scenarios. Thirdly, variance across different iterations is also lower. Although the variance across all metrics is generally low (except for $R^2$), RBO exhibits less variance than Kendall tau and Weighted Kendall tau, which could be a distinguishing factor between these three metrics. CC, despite having a low variance (around 0.01), still shows higher variance compared to the other metrics (except $R^2$), which have variances close to zero. This is important because it allows for stable results regardless of the number of runs.

Regarding text-based metrics, there is a sharp drop when only one column is mutated (Figure~\ref{fig:lineplot}), but the decline is steady thereafter. As for $R^2$, it is not a suitable metric for this type of problem, as it is unbounded between 0 and 1 and exhibits high variance across iterations. This is likely due to its design as a regression metric, which is not suited for this particular problem.

While proposed metrics such as RBO and Kendall tau show promise in capturing changes in feature importance, several limitations should be addressed in future research. Firstly, synthetic datasets may differ in ways that a simple value permutation cannot capture. For instance, the distribution of the data or the number of unique values may vary. Nevertheless, we tested three synthetic datasets generated by us as counterparts to the five original datasets, performing a sanity check to ensure that the new approaches align with \ac{cc} for synthetic datasets (Figure~\ref{fig:synth_result}). The results show that the values are at least close to those of CC, and they seem to exhibit less variability, which is reasonable given that the synthetic data generator was the same. These results suggest that the new approaches could be good alternatives to CC.

Secondly, the metrics have primarily been tested on synthetic datasets, and their performance on real-world datasets with varying distributions, noise, and missing data remains to be fully explored. Thirdly, these metrics may not fully account for complex interactions between features, which are often present in real-world datasets. Future research could focus on incorporating feature interaction measures or combining these metrics with statistical distance metrics to better capture such complexities. Additionally, although the variance of the proposed metrics is generally low, further research into methods for reducing variance, particularly in high-dimensional datasets, would enhance their robustness. Finally, developing methods to provide confidence intervals or uncertainty estimates for these metrics would improve their interpretability and reliability in practical applications.

Regarding column types and variables, we note in figure~\ref{fig:imbalance} that at least one category was highly imbalanced throughout the process. Imbalanced data clearly affects the data generators, but more importantly, it introduces challenges when creating \ac{cc} metrics. This must be considered when training and testing models, especially when encoding—such as making exceptions for unknown categories—and, more importantly, when using these variables as target variables, with particular attention to stratification of categories. Additionally, the metrics applied may also be impacted.

Examining the impact of the columns, Table~\ref{tab:method_column_type} shows that the column type significantly affects the metric. This is expected, as metrics for regression are not bound by the 0-1 range. Even for categorical data, ratios higher than 1 can be observed when the model performs better on synthetic data than on the real dataset. This phenomenon occurs more easily and with greater impact in regression. While this suggests that \ac{cc} also shares this limitation, it highlights the need for further research to identify which metrics and target columns are most suitable for exploration. Furthermore, considering that most real-world datasets consist of mixed data types, an effective \ac{cc} method should include regression tasks and encompass several target variables. We also explored the inverse of our main approach, comparing results from testing on synthetic datasets with those from testing on both real and synthetic datasets. The behaviour was similar to the original approach, and no significant differences were observed.

In light of these findings, we believe progress has been made in comparing tabular datasets, which may prove beneficial not only for evaluating synthetic data generators but also for formalising the degree of similarity between datasets that share common features. This approach provides a valuable reference for the development of new data generators and strengthens confidence in benchmarks and comparisons.