% !TeX root = ../../thesis.tex


With the results found, we feel that are better alternatives to CC. \ac{rbo} seems like alternatives to CC. Firstly, as with Kendall tau and Weighted Kendall tau, it seems to be directly connected to a difference in the dataset. They seem to lower from 1 (equal datasets) to around 50\% when the columns changed are half of the columns of the dataset, which seems an encouraging result. Secondly, it is a bounded metric between 0 and 1, which is helpful for interpretation since, as we have seen with for example CC and $R^2$, they can go outside these bounds, making  the interpretation of the results more difficult for these scenarios. Thirdly, variance across different iterations is also lower. Even though the variance across all metrics is quite low (except $R^2$), \ac{rbo} seems to have less variance than Kendall tau and Weighted Kendall tau, which could be the differentiating factor between these 3 metrics.  With CC, we have a variance that despite low (on the 0.01 level), is still higher than the other metrics (except $R^2$) which have variance around 0. This is important, since it is independent of numbers of runs to get a stable result.
Regarding text metrics, they have a drastic drop with only one column mutated (Figure~\ref{fig:lineplot}) but the decrease is steady afterwards. As for $R^2$, we see that it is not a good metric for this type of problem, since it is not bounded between 0 and 1, and it has a high variance across different iterations. This is probably due to the fact that it is a regression metric, and it is not designed for this type of problem.

While the proposed metrics such as \ac{rbo} and Kendall Tau have shown promise in capturing changes in feature importance, there are several limitations that should be addressed in future research. 
Firstly, the possibility of synthetic datasets may be different in other ways that a value permutation could not grasp. For example, the distribution of the data could be different, or the number of unique values could be different. Nevertheless, we tested with 3 sets of synthetic datasets produced by us as counterpart to the 5 original datasets to make at least a sanity check that the new approaches assessed are at least in tune with CC for actual synthetic datasets (Figure~\ref{fig:synth_result} and \ref{fig:synth_heat}). We can see that the values are at least near the values of CC. Moreover, they seem to have less variability, which can make sense taken into account the synthetic data generator was the same. These results also point towards the direction that these new approaches could be a good alternative to CC.

Secondly, the metrics have primarily been tested on synthetic datasets, and their performance on real-world datasets with varying distributions, noise, and missing data remains to be fully explored. Thirdly, these metrics may not fully account for complex interactions between features, which are often present in real-world datasets. Future work could focus on incorporating feature interaction measures or combining these metrics with statistical distance metrics to better reflect such complexities. Additionally, while the variance of the proposed metrics is generally low, exploring methods to further reduce variance, especially in high-dimensional datasets, would enhance their robustness. Finally, future research could develop methods to provide confidence intervals or uncertainty estimates for these metrics, which would improve their interpretability and reliability in practical applications.


With these findings, we believe that progress has been made in comparing tabular datasets, which may prove beneficial not only for evaluating synthetic data generators but also for formalizing the degree of similarity between datasets that share common features. This approach provides a valuable reference for the development of new data generators and enhances confidence in benchmarks and comparisons.