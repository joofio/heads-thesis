% !TeX root = ../../thesis.tex

In recent years, the use of \ac{ai} and \ac{ml} algorithms has gained increasing prominence in healthcare research and practice. One of the key requirements for the successful application of these methods is access to large, high-quality datasets. However, in many cases, the availability of such datasets can be limited due to issues around data privacy, security, and ethical concerns \cite{chingOpportunitiesObstaclesDeep2018a}. To address this challenge, synthetic data has emerged as a promising solution. 

Synthetic data refers to artificially generated data that closely mimic the statistical properties and patterns of real-world data \cite{mullerEvaluationSyntheticElectronic2022} and has many applications in healthcare. For instance, in clinical research, synthetic data can simulate patient responses to a drug, enabling researchers to conduct preliminary analyses and identify potential outcomes without risking actual patient health. In training machine learning models, it's used to augment datasets, improving the accuracy and robustness of predictive models without exposing sensitive patient information. Hospitals and healthcare providers use synthetic data for resource planning and management, creating virtual scenarios to optimize staff allocation and equipment usage. Additionally, in medical imaging, synthetic data helps in developing more accurate diagnostic tools by providing a diverse range of images, which might be scarce in real datasets, particularly for rare conditions. Each of these examples showcases the versatility and potential of synthetic data in enhancing healthcare services while safeguarding patient confidentiality.

So, with this, synthetic data offers a promising solution to several challenges inherent in real-world data, including limited data volume and privacy issues. While there are ongoing debates about its effectiveness as a comprehensive solution for privacy concerns \cite{stadlerSyntheticDataPrivacy2020}, its application in data upsampling has been a well-established practice for years. The effectiveness of synthetic data generators varies considerably, making it critical to evaluate the resemblance between synthetic and real data prior to utilization. This notion of similarity, or utility, is pivotal in unlocking the full potential of synthetic data. Without understanding the degree of similarity, it's challenging to gauge its utility. Particularly in healthcare, rigorously assessing synthetic data is essential to confirm its validity in offering insightful contributions and informing decision-making processes. %reference

Yet, the crux lies in guaranteeing the high quality and validation of synthetic data to ensure it provides dependable and meaningful insights. Quality evaluation of synthetic data involves a thorough comparison of its statistical attributes and patterns against those of the original dataset. The current state-of-the-art quality assessment of synthetic data involves a detailed comparison of its statistical characteristics and patterns with those of the original dataset. This includes examining the similarity of columns through various statistical tests and exploring inter-column relationships using methods like cross-classification, where two datasets are divided into training and test sets, cross-tested, and the ratio of their evaluation results is used as a metric \cite{mullerEvaluationSyntheticElectronic2022, goncalvesGenerationEvaluationSynthetic2020a}. However, this approach is somewhat limited in capturing the nuances of inter-column relationships. The challenge then is to develop a more nuanced metric that better represents the intricacies of these relationships in two distinct datasets. Additionally, since the metric is ratio-based, it is not constrained by a specific range and can exceed 1, which complicates the interpretation of the results. 

To address these issues, this paper proposes the use of ML models' feature importance values as a basis for a more comprehensive and robust metric to evaluate the similarity of inter-column relationships and overall dataset congruence.