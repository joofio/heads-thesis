% !TeX root = ../../thesis.tex

In recent years, the use of \ac{ai} and \ac{ml} algorithms has gained increasing prominence in healthcare research and practice. One of the key requirements for the successful application of these methods is access to large, high-quality datasets. However, in many cases, the availability of such datasets can be limited due to issues around data privacy, security, and ethical concerns \cite{chingOpportunitiesObstaclesDeep2018a}. To address this challenge, synthetic data has emerged as a promising solution. Synthetic data refers to artificially generated data that closely mimic the statistical properties and patterns of real-world data \cite{mullerEvaluationSyntheticElectronic2022}.

Synthetic data has the potential to overcome many of the limitations associated with real-world data, such as the lack of sufficient data volume, noise, and privacy concerns. Even though there are still doubts if the privacy part is the silver bullet sometimes referred to \cite{stadlerSyntheticDataPrivacy2020}, the upsampling part is a standard use for years now. However, the quality of synthetic data generated by various techniques can vary significantly, and it is essential to assess the quality of synthetic data before its usage. In healthcare, the assessment of synthetic data is crucial to ensure that it can provide valid insights and inform decision-making processes.

The assessment of synthetic data in healthcare is essential for its successful use in various applications, such as developing predictive models, testing algorithms, and conducting clinical trials. The use of synthetic data can significantly enhance the efficiency and effectiveness of healthcare research and practice. However, it is crucial to ensure that the synthetic data used in these applications are of high quality and validated to provide reliable and valid insights. The evaluation of synthetic data quality involves comparing its statistical properties and patterns with those of the original data. We can assess how similar columns are to each other through several statistical tests, and then we can infer some inter-column properties with methods like cross-validation, where two datasets are split into train tests and cross-tested and then the ratio between the evaluation result of both datasets is used as a metric \cite{mullerEvaluationSyntheticElectronic2022,goncalvesGenerationEvaluationSynthetic2020a}. However, this methodology is a big proxy for such an inter-column relationship. Can we try to provide a better metric than this one to evaluate how similar are the inter-column relationship of two distinct datasets? In this paper, we suggest using feature importance values to create a more explainable and reasonable metric for inter-column relationships.
